# ğŸ¤– RAG Stock Chatbot

> Intelligent chatbot for Vietnamese Stock Market Q&A using Retrieval-Augmented Generation (RAG)

## ğŸ“‹ Overview

**RAG Stock Chatbot** is an AI-powered conversational assistant designed to answer questions about the Vietnamese stock market. It leverages Retrieval-Augmented Generation (RAG) architecture with LangChain, Google Gemini LLM, and multiple vector stores to provide accurate, contextual responses based on real stock market news and data.

## âœ¨ Key Features

- ğŸ§  **RAG Architecture** - Retrieves relevant documents before generating answers
- ğŸ’¬ **Conversational Memory** - Maintains chat history using Redis
- ğŸ” **Multi-Embedding Support**
  - BiEncoder embeddings
  - PhoBERT (Vietnamese BERT)
  - Sentence-BERT
- ğŸ“Š **Vector Search** - Qdrant vector database for semantic search
- ğŸ—„ï¸ **Document Storage** - MongoDB for document persistence
- ğŸŒ **RESTful API** - FastAPI endpoints for easy integration
- ğŸ³ **Dockerized** - Complete Docker Compose setup
- ğŸ‡»ğŸ‡³ **Vietnamese Language** - Optimized for Vietnamese stock market terminology

## ğŸ› ï¸ Technology Stack

| Component | Technology |
|-----------|------------|
| **LLM** | Google Gemini 1.5 Pro |
| **Framework** | LangChain |
| **API** | FastAPI |
| **Vector DB** | Qdrant |
| **Document Store** | MongoDB |
| **Cache/Memory** | Redis |
| **Embeddings** | PhoBERT, Sentence-Transformers |
| **Containerization** | Docker, Docker Compose |

## ğŸ“ Project Structure

```
rag-stock-chatbot/
â”œâ”€â”€ chatbot_rag/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ app.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ chatbot.py          # Main chatbot logic
â”‚   â”‚   â”œâ”€â”€ rag_chain.py        # RAG chain configuration
â”‚   â”‚   â”œâ”€â”€ retriever.py        # Document retriever
â”‚   â”‚   â”œâ”€â”€ store.py            # Vector & document stores
â”‚   â”‚   â”œâ”€â”€ embedding.py        # Embedding models
â”‚   â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”‚   â””â”€â”€ Dockerfile          # App container config
â”‚   â””â”€â”€ docker-compose.yaml     # Multi-container setup
â”œâ”€â”€ CreatImage/                 # Image creation utilities
â”œâ”€â”€ Data/                       # Stock market datasets
â”‚   â”œâ”€â”€ data_thanhnien.csv
â”‚   â”œâ”€â”€ data_tnck.csv
â”‚   â”œâ”€â”€ data_VNE.csv
â”‚   â””â”€â”€ data_vne1.csv
â””â”€â”€ README.md
```

## ğŸ—ƒï¸ Architecture

### RAG Pipeline

```
User Question
    â†“
History-Aware Retriever
    â†“
Vector Search (Qdrant)
    â†“
Retrieved Documents
    â†“
LLM (Gemini) + Context
    â†“
Generated Answer
```

### Components

1. **Retriever** - Fetches relevant documents from vector store
2. **RAG Chain** - Combines retrieval and generation
3. **Chat History** - Redis-based conversation memory
4. **Vector Store** - Qdrant with multiple embedding models
5. **Document Store** - MongoDB for raw documents

## ğŸš€ Getting Started

### Prerequisites
- Docker & Docker Compose
- Python 3.9+
- Google Gemini API Key

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd rag-stock-chatbot/chatbot_rag
```

2. **Set up environment variables**
Create `.env` file:
```env
API_KEY=your_gemini_api_key_here
MONGO_URL=mongodb://mongodb:27017
QDRANT_URL=http://qdrant_db:6333
REDIS_URL=redis://redis:6379
```

3. **Start services with Docker Compose**
```bash
docker-compose up -d
```

This will start:
- **Qdrant** (port 6333) - Vector database
- **MongoDB** (port 27017) - Document store
- **Redis** (port 6379) - Chat history cache
- **FastAPI App** (port 5123) - Chatbot API

### Running without Docker

1. **Install dependencies**
```bash
pip install -r app/requirements.txt
```

2. **Start individual services**
```bash
# Start Qdrant, MongoDB, Redis separately
# Then run the app
python app/app.py
```

## ğŸ“¡ API Endpoints

### Chat with Bot
```http
POST /chat/{user_id}/{session_id}?question=your_question
```

**Parameters:**
- `user_id`: Unique user identifier
- `session_id`: Conversation session ID
- `question`: User's question

**Response:** Streaming text response

### Get Chat History
```http
GET /chat/history/{user_id}/{session_id}
```

**Response:** Full conversation history

### Clear Chat History
```http
POST /chat/clear/{user_id}/{session_id}
```

**Response:** Status confirmation

### Insert Data
```http
PUT /insert_data?csv_path=/path/to/data.csv
```

**Response:** Data insertion status

## ğŸ’¡ Usage Examples

### Python Client
```python
import requests

# Ask a question
response = requests.post(
    "http://localhost:5123/chat/user123/session456",
    params={"question": "GiÃ¡ cá»• phiáº¿u VNM hÃ´m nay nhÆ° tháº¿ nÃ o?"}
)

# Stream the response
for chunk in response.iter_content(chunk_size=1024):
    print(chunk.decode('utf-8'), end='')
```

### cURL
```bash
# Ask question
curl -X POST "http://localhost:5123/chat/user123/session456?question=Thá»‹%20trÆ°á»ng%20chá»©ng%20khoÃ¡n%20hÃ´m%20nay"

# Get history
curl -X GET "http://localhost:5123/chat/history/user123/session456"

# Clear history
curl -X POST "http://localhost:5123/chat/clear/user123/session456"
```

## ğŸ”§ Configuration

### RAG Chain Settings
```python
# rag_chain.py
MODEL_NAME = 'gemini-1.5-pro-latest'
TEMPERATURE = 0.7
MAX_RETRIES = 6
```

### Chatbot Settings
```python
# chatbot.py
COLLECTION_NAME = 'stock-news'
HISTORY_WINDOW_K = 10  # Last 10 messages in context
```

### Embedding Models
- **BiEncoder**: For dense retrieval
- **PhoBERT**: Vietnamese BERT model
- **Sentence-BERT**: Multilingual embeddings

## ğŸ“Š Data Sources

The chatbot uses Vietnamese stock market news from:
- Thanh NiÃªn (data_thanhnien.csv)
- Tuá»•i Tráº» Chá»©ng KhoÃ¡n (data_tnck.csv)
- VNExpress (data_VNE.csv, data_vne1.csv)

### Data Format
```csv
title,content,link,date,source
```

## ğŸ¯ Key Features Explained

### History-Aware Retrieval
- Maintains conversation context
- Reformulates queries based on chat history
- Provides coherent multi-turn conversations

### Multi-Vector Store
- Three separate vector stores with different embeddings
- Allows comparison and selection of best embedding model
- Optimized for Vietnamese language

### Streaming Responses
- Real-time response generation
- Better user experience
- Reduced perceived latency

## ğŸ³ Docker Services

### Qdrant
```yaml
ports: 6333:6333, 6334:6334
volumes: ./qdrant_data:/qdrant_data
```

### MongoDB
```yaml
ports: 27017:27017
volumes: ./mongo_data:/data
```

### Redis
```yaml
ports: 6379:6379
volumes: ./redis_data:/data
```

## ğŸ” Troubleshooting

### Common Issues

1. **Connection errors**
   - Ensure all Docker containers are running
   - Check network connectivity between services

2. **API Key errors**
   - Verify Gemini API key is correct
   - Check API quota limits

3. **Memory issues**
   - Increase Docker memory allocation
   - Reduce embedding model size

4. **Slow responses**
   - Check vector store index size
   - Optimize chunk size in text splitters

## ğŸ“ˆ Performance Tips

- Use appropriate chunk sizes for text splitting
- Implement caching for frequent queries
- Monitor vector store size and optimize regularly
- Use streaming for better UX

## ğŸ“ License

This project is developed for educational and research purposes.

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Report bugs
- Suggest new features
- Submit pull requests
- Improve documentation

## ğŸ“š References

- [LangChain Documentation](https://python.langchain.com/)
- [Qdrant Vector Database](https://qdrant.tech/)
- [Google Gemini API](https://ai.google.dev/)
- [PhoBERT](https://github.com/VinAIResearch/PhoBERT)

---
â­ If you find this project useful, please give it a star!
